# Common imports (always needed)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# Framework-specific imports based on model config
{% if framework == "sklearn" %}
{% if model_config %}{{ model_config.import }}{% endif %}
{% elif framework == "tensorflow" %}
import tensorflow as tf
from tensorflow import keras
{% elif framework == "pytorch" %}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
{% elif framework == "xgboost" %}
import xgboost as xgb
{% elif framework == "lightgbm" %}
import lightgbm as lgb
{% elif framework == "catboost" %}
from catboost import CatBoostClassifier
{% endif %}

# Config from prompt
model_name = "{{ model }}"
dataset = "{{ dataset }}"
optimizer = "{{ optimizer }}"
lr = {{ learning_rate | default(0.001) | float }}
epochs = {{ epochs | default(10) }}

print(f"Training {model_name} on {dataset} for {epochs} epochs using {{ framework }}...")

# Generic dataset loading logic
try:
    df = pd.read_csv(f"data/{dataset}.csv")
    print(f"Loaded dataset from data/{dataset}.csv")
except Exception as e:
    try:
        import seaborn as sns
        df = sns.load_dataset(dataset).dropna()
        print(f"Loaded {dataset} from seaborn datasets")
    except Exception as e2:
        print(f"Error loading dataset: {e2}")
        exit(1)

# Preprocessing
label_encoders = {}
for col in df.select_dtypes(include=["object", "category"]).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Assume last column is target
X = df.drop(columns=[df.columns[-1]])
y = df[df.columns[-1]]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features for certain models
{% if framework in ["tensorflow", "pytorch"] or model in ["svm", "logistic_regression"] %}
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train, X_test = X_train_scaled, X_test_scaled
{% endif %}

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

# Training based on framework and model
{% if framework == "sklearn" %}
# Sklearn model training
{% if model_config %}
model_params = {}
{% for key, value in model_config.default_params.items() %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endfor %}

# Apply user overrides
{% for key, value in other_params.items() %}
{% if key in model_config.default_params.keys() %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endif %}
{% endfor %}

print(f"Model parameters: {model_params}")
model = {{ model_config.class }}(**model_params)
{% else %}
# Fallback for missing model config
print("Warning: Model config not found, using basic parameters")
model_params = {}
{% for key, value in other_params.items() %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endfor %}
print(f"Model parameters: {model_params}")

# Try to import and create model dynamically
if "{{ model }}" == "random_forest":
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(**model_params)
elif "{{ model }}" == "logistic_regression":
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(**model_params)
elif "{{ model }}" == "svm":
    from sklearn.svm import SVC
    model = SVC(**model_params)
elif "{{ model }}" == "gradient_boosting":
    from sklearn.ensemble import GradientBoostingClassifier
    model = GradientBoostingClassifier(**model_params)
else:
    print(f"Unknown model: {{ model }}")
    exit(1)
{% endif %}

model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

{% elif framework == "xgboost" %}
# XGBoost training
model_params = {
    'max_depth': {{ other_params.get('max_depth', 3) }},
    'learning_rate': lr,
    'n_estimators': epochs,
    'random_state': 42
}
# Apply additional user parameters
{% for key, value in other_params.items() %}
{% if key not in ['max_depth', 'learning_rate', 'n_estimators', 'random_state'] %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endif %}
{% endfor %}

print(f"XGBoost parameters: {model_params}")
model = xgb.XGBClassifier(**model_params)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

{% elif framework == "lightgbm" %}
# LightGBM training  
model_params = {
    'n_estimators': epochs,
    'learning_rate': lr,
    'random_state': 42
}
{% for key, value in other_params.items() %}
{% if key not in ['n_estimators', 'learning_rate', 'random_state'] %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endif %}
{% endfor %}

print(f"LightGBM parameters: {model_params}")
model = lgb.LGBMClassifier(**model_params)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

{% elif framework == "catboost" %}
# CatBoost training
model_params = {
    'iterations': epochs,
    'learning_rate': lr,
    'verbose': 0,
    'logging_level': 'Silent',
    'train_dir': 'outputs/.catboost',
    'random_state': 42
}
{% for key, value in other_params.items() %}
{% if key not in ['iterations', 'learning_rate', 'verbose', 'logging_level', 'train_dir', 'random_state'] %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endif %}
{% endfor %}

print(f"CatBoost parameters: {model_params}")
model = CatBoostClassifier(**model_params)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

{% elif framework == "tensorflow" %}
# TensorFlow/Keras model building
{% if model_config.architecture == "mlp" or model_config.architecture == "deep" %}
model = keras.Sequential()

# Add input layer
model.add(keras.layers.Dense({{ model_config.layers[0].units }}, 
                           activation='{{ model_config.layers[0].activation }}',
                           input_shape=(X_train.shape[1],)))

# Add hidden layers based on config
{% for layer in model_config.layers[1:] %}
{% if layer.type == "dense" %}
model.add(keras.layers.Dense({{ layer.units }}, activation='{{ layer.activation }}'))
{% elif layer.type == "dropout" %}
model.add(keras.layers.Dropout({{ layer.rate }}))
{% endif %}
{% endfor %}

{% else %}
# Default simple model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
{% endif %}

# Compile model
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=lr),
    loss='binary_crossentropy' if len(np.unique(y)) == 2 else 'sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("Model architecture:")
model.summary()

# Train model
history = model.fit(
    X_train, y_train,
    epochs=epochs,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Evaluate
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {test_accuracy:.4f}")

{% elif framework == "pytorch" %}
# PyTorch model building
{% if model_config.architecture == "cnn" %}
# CNN for tabular data (treating features as 1D sequence)
class TabularCNN(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        x = x.unsqueeze(1)  # Add channel dimension
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = x.squeeze(-1)
        x = self.fc(x)
        return self.sigmoid(x)

model = TabularCNN(X_train.shape[1])
{% else %}
# Default MLP
class MLP(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.layers(x)

model = MLP(X_train.shape[1])
{% endif %}

# Prepare data
X_tensor = torch.tensor(X_train, dtype=torch.float32)
y_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)
dataset = TensorDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Loss and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

print(f"Model: {model}")
print(f"Training on {len(dataloader)} batches...")

# Training loop
model.train()
for epoch in range(epochs):
    epoch_loss = 0
    for batch_idx, (data, target) in enumerate(dataloader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(dataloader)
    print(f"Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}")

# Evaluation
model.eval()
with torch.no_grad():
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_pred = model(X_test_tensor)
    y_pred_binary = (y_pred > 0.5).float().squeeze()
    accuracy = (y_pred_binary == torch.tensor(y_test.values, dtype=torch.float32)).float().mean()
    print(f"Test Accuracy: {accuracy:.4f}")

{% endif %}

print("\nTraining completed successfully!")

# Save model info
import json
import os

# Create outputs directory if it doesn't exist
os.makedirs("outputs", exist_ok=True)

# Prepare model info
model_info = {
    "model_name": model_name,
    "framework": "{{ framework }}",
    "dataset": dataset,
}

{% if model_config %}
# Add model config if available
config_data = {
    "class": "{{ model_config.class }}",
    "framework": "{{ model_config.framework }}",
    "import": "{{ model_config.import }}",
    "default_params": {
        {% for key, value in model_config.default_params.items() %}
        "{{ key }}": {{ "None" if value is none else (value if value is number else '"' + value|string + '"') }},
        {% endfor %}
    }
}
model_info["config"] = config_data
{% else %}
model_info["config"] = None
{% endif %}

with open(f"outputs/model_info_{model_name}.json", "w", encoding='utf-8') as f:
    json.dump(model_info, f, indent=2)

print(f"Model info saved to outputs/model_info_{model_name}.json")