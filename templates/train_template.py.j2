# Common imports (always needed)
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score

# Cross-validation imports
{% if use_cv %}
from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, cross_validate
from sklearn.metrics import make_scorer
{% else %}
from sklearn.model_selection import train_test_split
{% endif %}

# Visualization imports
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from core.visualizer import create_visualizations
    VISUALIZATIONS_AVAILABLE = True
    print("üé® Visualizations enabled")
except ImportError as e:
    VISUALIZATIONS_AVAILABLE = False
    print(f"‚ö†Ô∏è  Visualizations not available: {e}")

# Framework-specific imports based on model config
{% if framework == "sklearn" %}
{% if model_config %}{{ model_config.import }}{% endif %}
{% elif framework == "tensorflow" %}
import tensorflow as tf
from tensorflow import keras
{% if use_cv %}
from sklearn.model_selection import StratifiedKFold, KFold
{% endif %}
{% elif framework == "pytorch" %}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
{% if use_cv %}
from sklearn.model_selection import StratifiedKFold, KFold
{% endif %}
{% elif framework == "xgboost" %}
import xgboost as xgb
{% elif framework == "lightgbm" %}
import lightgbm as lgb
{% elif framework == "catboost" %}
from catboost import CatBoostClassifier
{% endif %}

# Config from prompt
model_name = "{{ model }}"
dataset = "{{ dataset }}"
optimizer = "{{ optimizer }}"
lr = {{ learning_rate | default(0.001) | float }}
epochs = {{ epochs | default(10) }}
task_type = "{{ task_type | default('classification') }}"
use_cv = {{ use_cv | default(False) | python_bool }}
cv_folds = {{ cv_folds | default(5) }}
cv_type = "{{ cv_type | default('auto') }}"

print(f"Training {model_name} on {dataset} dataset")
print(f"Task type: {task_type}")
if use_cv:
    print(f"Using {cv_folds}-fold cross-validation ({cv_type})")
else:
    print("Using train/test split")
print(f"Framework: {{ framework }}")

# Generic dataset loading logic
try:
    df = pd.read_csv(f"data/{dataset}.csv")
    print(f"[SUCCESS] Loaded dataset from data/{dataset}.csv")
except Exception as e:
    try:
        import seaborn as sns
        df = sns.load_dataset(dataset).dropna()
        print(f"[SUCCESS] Loaded {dataset} from seaborn datasets")
    except Exception as e2:
        try:
            from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits, load_diabetes
            dataset_loaders = {
                'iris': load_iris,
                'wine': load_wine, 
                'breast_cancer': load_breast_cancer,
                'digits': load_digits,
                'diabetes': load_diabetes
            }
            
            if dataset.lower() in dataset_loaders and dataset_loaders[dataset.lower()]:
                sklearn_data = dataset_loaders[dataset.lower()]()
                df = pd.DataFrame(sklearn_data.data, columns=sklearn_data.feature_names if hasattr(sklearn_data, 'feature_names') else [f'feature_{i}' for i in range(sklearn_data.data.shape[1])])
                df['target'] = sklearn_data.target
                print(f"[SUCCESS] Loaded {dataset} from sklearn datasets")
            else:
                print(f"[ERROR] Dataset '{dataset}' not found. Available: iris, wine, breast_cancer, digits, diabetes")
                exit(1)
        except Exception as e3:
            print(f"[ERROR] Error loading dataset: {e3}")
            exit(1)

# Preprocessing
print(f"Dataset shape: {df.shape}")
print(f"Dataset columns: {list(df.columns)}")

# Handle categorical columns
label_encoders = {}
for col in df.select_dtypes(include=["object", "category"]).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Assume last column is target (or 'target' column if exists)
if 'target' in df.columns:
    X = df.drop(columns=['target'])
    y = df['target']
else:
    X = df.drop(columns=[df.columns[-1]])
    y = df[df.columns[-1]]

# Auto-detect task type if not specified
if task_type == "auto":
    unique_targets = len(y.unique())
    if unique_targets <= 20 and y.dtype in ['int64', 'object', 'category']:
        task_type = "classification"
        print(f"[INFO] Auto-detected task type: classification ({unique_targets} classes)")
    else:
        task_type = "regression"
        print(f"[INFO] Auto-detected task type: regression (continuous target)")

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
if task_type == "classification":
    print(f"Classes: {sorted(y.unique())}")
else:
    print(f"Target range: [{y.min():.3f}, {y.max():.3f}]")

# Scale features for certain models
{% if framework in ["tensorflow", "pytorch"] or model in ["svm", "logistic_regression"] %}
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)
print("[SUCCESS] Features scaled using StandardScaler")
{% endif %}

# Cross-validation setup
{% if use_cv %}
# Determine CV strategy
if cv_type == "auto":
    if task_type == "classification":
        cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        print(f"[SUCCESS] Using StratifiedKFold with {cv_folds} folds")
    else:
        cv_strategy = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
        print(f"[SUCCESS] Using KFold with {cv_folds} folds")
elif cv_type == "stratified":
    cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    print(f"[SUCCESS] Using StratifiedKFold with {cv_folds} folds")
else:  # kfold
    cv_strategy = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    print(f"[SUCCESS] Using KFold with {cv_folds} folds")

# Define scoring metric
if task_type == "classification":
    scoring = 'accuracy'
    scoring_name = "Accuracy"
else:
    scoring = 'neg_mean_squared_error'
    scoring_name = "Negative MSE"

{% else %}
# Traditional train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, 
                                                   stratify=y if task_type == "classification" else None)
print(f"[SUCCESS] Train/test split: {X_train.shape[0]} train, {X_test.shape[0]} test samples")
{% endif %}

# Training based on framework and model
{% if framework == "sklearn" %}
# Sklearn model training
{% if model_config %}
model_params = {}
{% for key, value in model_config.default_params.items() %}
model_params["{{ key }}"] = {{ value | python_value }}
{% endfor %}

# Apply user overrides
{% for key, value in other_params.items() %}
{% if key in model_config.default_params.keys() %}
model_params["{{ key }}"] = {{ value | python_value }}
{% endif %}
{% endfor %}

print(f"[SUCCESS] Model parameters: {model_params}")
model = {{ model_config.class }}(**model_params)
{% else %}
# Fallback for missing model config
print("[WARNING] Model config not found, using basic parameters")
model_params = {}
{% for key, value in other_params.items() %}
model_params["{{ key }}"] = {{ value | python_value }}
{% endfor %}
print(f"[SUCCESS] Model parameters: {model_params}")

# Try to import and create model dynamically
if "{{ model }}" == "random_forest":
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(**model_params)
elif "{{ model }}" == "logistic_regression":
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(**model_params)
elif "{{ model }}" == "svm":
    from sklearn.svm import SVC
    model = SVC(**model_params)
elif "{{ model }}" == "gradient_boosting":
    from sklearn.ensemble import GradientBoostingClassifier
    model = GradientBoostingClassifier(**model_params)
else:
    print(f"[ERROR] Unknown model: {{ model }}")
    exit(1)
{% endif %}

{% if use_cv %}
# Cross-validation training
print(f"\n[INFO] Starting {cv_folds}-fold cross-validation...")

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=cv_strategy, scoring=scoring, n_jobs=-1)

# Convert negative MSE back to positive for regression
if task_type == "regression" and scoring == 'neg_mean_squared_error':
    cv_scores = -cv_scores
    scoring_name = "MSE"

mean_score = cv_scores.mean()
std_score = cv_scores.std()

print(f"\n[RESULTS] Cross-Validation Results:")
print(f"Individual fold scores: {[f'{score:.4f}' for score in cv_scores]}")
print(f"Mean CV {scoring_name}: {mean_score:.4f} ¬± {std_score:.4f}")

# Also get additional metrics
if task_type == "classification":
    cv_results = cross_validate(model, X, y, cv=cv_strategy, 
                               scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],
                               n_jobs=-1)
    
    print(f"\n[METRICS] Additional CV Metrics:")
    for metric, scores in cv_results.items():
        if metric.startswith('test_'):
            metric_name = metric.replace('test_', '').replace('_', ' ').title()
            print(f"{metric_name}: {scores.mean():.4f} ¬± {scores.std():.4f}")

# Train on full dataset for final model
print(f"\n[INFO] Training final model on full dataset...")
model.fit(X, y)

{% else %}
# Traditional training
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)

if task_type == "classification":
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n[RESULTS] Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"\n[REPORT] Classification Report:")
    print(classification_report(y_test, y_pred))
else:
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mse)
    print(f"\n[RESULTS] Results:")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R¬≤: {r2:.4f}")
{% endif %}

{% elif framework == "xgboost" %}
# XGBoost training
model_params = {
    'max_depth': {{ other_params.get('max_depth', 3) }},
    'learning_rate': lr,
    'n_estimators': epochs,
    'random_state': 42,
    'objective': 'binary:logistic' if task_type == 'classification' and len(y.unique()) == 2 else ('multi:softprob' if task_type == 'classification' else 'reg:squarederror')
}

# Apply additional user parameters
{% for key, value in other_params.items() %}
{% if key not in ['max_depth', 'learning_rate', 'n_estimators', 'random_state', 'objective'] %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endif %}
{% endfor %}

print(f"[SUCCESS] XGBoost parameters: {model_params}")

if task_type == "classification":
    model = xgb.XGBClassifier(**model_params)
else:
    model = xgb.XGBRegressor(**model_params)

{% if use_cv %}
# XGBoost Cross-validation
print(f"\n[INFO] Starting {cv_folds}-fold cross-validation...")
cv_scores = cross_val_score(model, X, y, cv=cv_strategy, scoring=scoring, n_jobs=-1)

if task_type == "regression" and scoring == 'neg_mean_squared_error':
    cv_scores = -cv_scores

mean_score = cv_scores.mean()
std_score = cv_scores.std()
print(f"Mean CV {scoring_name}: {mean_score:.4f} ¬± {std_score:.4f}")

# Train final model
model.fit(X, y)
{% else %}
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

if task_type == "classification":
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
else:
    print(f"MSE: {mean_squared_error(y_test, y_pred):.4f}")
    print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
{% endif %}

{% elif framework == "lightgbm" %}
# LightGBM training  
model_params = {
    'n_estimators': epochs,
    'learning_rate': lr,
    'random_state': 42,
    'objective': 'binary' if task_type == 'classification' and len(y.unique()) == 2 else ('multiclass' if task_type == 'classification' else 'regression')
}

{% for key, value in other_params.items() %}
{% if key not in ['n_estimators', 'learning_rate', 'random_state', 'objective'] %}
model_params["{{ key }}"] = {{ "None" if value is none else (value if value is number else "'" + value|string + "'") }}
{% endif %}
{% endfor %}

print(f"[SUCCESS] LightGBM parameters: {model_params}")

if task_type == "classification":
    model = lgb.LGBMClassifier(**model_params)
else:
    model = lgb.LGBMRegressor(**model_params)

{% if use_cv %}
print(f"\n[INFO] Starting {cv_folds}-fold cross-validation...")
cv_scores = cross_val_score(model, X, y, cv=cv_strategy, scoring=scoring, n_jobs=-1)

if task_type == "regression" and scoring == 'neg_mean_squared_error':
    cv_scores = -cv_scores

mean_score = cv_scores.mean()
std_score = cv_scores.std()
print(f"Mean CV {scoring_name}: {mean_score:.4f} ¬± {std_score:.4f}")

model.fit(X, y)
{% else %}
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

if task_type == "classification":
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
else:
    print(f"MSE: {mean_squared_error(y_test, y_pred):.4f}")
{% endif %}

{% elif framework == "catboost" %}
# CatBoost training
model_params = {
    'iterations': epochs,
    'learning_rate': lr,
    'verbose': 0,
    'logging_level': 'Silent',
    'train_dir': 'outputs/.catboost',
    'random_state': 42,
    'loss_function': 'Logloss' if task_type == 'classification' and len(y.unique()) == 2 else ('MultiClass' if task_type == 'classification' else 'RMSE')
}

{% for key, value in other_params.items() %}
{% if key not in ['iterations', 'learning_rate', 'verbose', 'logging_level', 'train_dir', 'random_state', 'loss_function'] %}
model_params["{{ key }}"] = {{ value | python_value }}
{% endif %}
{% endfor %}

print(f"[SUCCESS] CatBoost parameters: {model_params}")

if task_type == "classification":
    model = CatBoostClassifier(**model_params)
else:
    from catboost import CatBoostRegressor
    model = CatBoostRegressor(**model_params)

{% if use_cv %}
print(f"\n[INFO] Starting {cv_folds}-fold cross-validation...")
cv_scores = cross_val_score(model, X, y, cv=cv_strategy, scoring=scoring, n_jobs=-1)

if task_type == "regression" and scoring == 'neg_mean_squared_error':
    cv_scores = -cv_scores

mean_score = cv_scores.mean()
std_score = cv_scores.std()
print(f"Mean CV {scoring_name}: {mean_score:.4f} ¬± {std_score:.4f}")

model.fit(X, y)
{% else %}
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

if task_type == "classification":
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
else:
    print(f"MSE: {mean_squared_error(y_test, y_pred):.4f}")
{% endif %}

{% elif framework == "tensorflow" %}
# TensorFlow/Keras model building
{% if model_config.architecture == "mlp" or model_config.architecture == "deep" %}
model = keras.Sequential()

# Add input layer
model.add(keras.layers.Dense({{ model_config.layers[0].units }}, 
                           activation='{{ model_config.layers[0].activation }}',
                           input_shape=(X.shape[1],)))

# Add hidden layers based on config
{% for layer in model_config.layers[1:] %}
{% if layer.type == "dense" %}
model.add(keras.layers.Dense({{ layer.units }}, activation='{{ layer.activation }}'))
{% elif layer.type == "dropout" %}
model.add(keras.layers.Dropout({{ layer.rate }}))
{% endif %}
{% endfor %}

{% else %}
# Default simple model
if task_type == "classification":
    output_units = len(y.unique()) if len(y.unique()) > 2 else 1
    output_activation = 'softmax' if len(y.unique()) > 2 else 'sigmoid'
else:
    output_units = 1
    output_activation = 'linear'

model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(output_units, activation=output_activation)
])
{% endif %}

# Compile model
if task_type == "classification":
    loss = 'binary_crossentropy' if len(y.unique()) == 2 else 'sparse_categorical_crossentropy'
    metrics = ['accuracy']
else:
    loss = 'mse'
    metrics = ['mae']

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=lr),
    loss=loss,
    metrics=metrics
)

print("[SUCCESS] Model architecture:")
model.summary()

{% if use_cv %}
# Manual cross-validation for deep learning
print(f"\n[INFO] Starting {cv_folds}-fold cross-validation...")

cv_scores = []
X_array = X.values if hasattr(X, 'values') else X
y_array = y.values if hasattr(y, 'values') else y

for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X_array, y_array)):
    print(f"Training fold {fold + 1}/{cv_folds}...")
    
    X_fold_train, X_fold_val = X_array[train_idx], X_array[val_idx]
    y_fold_train, y_fold_val = y_array[train_idx], y_array[val_idx]
    
    # Create and compile model for this fold
    fold_model = keras.models.clone_model(model)
    fold_model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=lr),
        loss=loss,
        metrics=metrics
    )
    
    # Train
    history = fold_model.fit(
        X_fold_train, y_fold_train,
        epochs=epochs,
        batch_size=32,
        validation_data=(X_fold_val, y_fold_val),
        verbose=0
    )
    
    # Evaluate
    if task_type == "classification":
        _, fold_score = fold_model.evaluate(X_fold_val, y_fold_val, verbose=0)
    else:
        fold_score = fold_model.evaluate(X_fold_val, y_fold_val, verbose=0)[0]  # MSE
    
    cv_scores.append(fold_score)
    print(f"  Fold {fold + 1} {scoring_name}: {fold_score:.4f}")

mean_score = np.mean(cv_scores)
std_score = np.std(cv_scores)
print(f"\nMean CV {scoring_name}: {mean_score:.4f} ¬± {std_score:.4f}")

# Train final model on full data
print("\n[INFO] Training final model on full dataset...")
model.fit(X, y, epochs=epochs, batch_size=32, verbose=1)

{% else %}
# Train model
history = model.fit(
    X_train, y_train,
    epochs=epochs,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Evaluate
test_results = model.evaluate(X_test, y_test, verbose=0)
if task_type == "classification":
    print(f"Test Accuracy: {test_results[1]:.4f}")
else:
    print(f"Test MSE: {test_results[0]:.4f}")
{% endif %}

{% elif framework == "pytorch" %}
# PyTorch model building - simplified for template
class MLP(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, output_size)
        )
        
        if task_type == "classification" and output_size == 1:
            self.activation = nn.Sigmoid()
        elif task_type == "classification":
            self.activation = nn.Softmax(dim=1)
        else:
            self.activation = nn.Identity()
    
    def forward(self, x):
        x = self.layers(x)
        return self.activation(x)

if task_type == "classification":
    output_size = len(y.unique()) if len(y.unique()) > 2 else 1
else:
    output_size = 1

model = MLP(X.shape[1], output_size)
print(f"[SUCCESS] PyTorch model: {model}")

# Loss and optimizer
if task_type == "classification":
    if output_size == 1:
        criterion = nn.BCELoss()
    else:
        criterion = nn.CrossEntropyLoss()
else:
    criterion = nn.MSELoss()

optimizer_torch = optim.Adam(model.parameters(), lr=lr)

{% if use_cv %}
# Manual cross-validation for PyTorch
print(f"\n[INFO] Starting {cv_folds}-fold cross-validation...")

cv_scores = []
X_array = X.values if hasattr(X, 'values') else X
y_array = y.values if hasattr(y, 'values') else y

for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X_array, y_array)):
    print(f"Training fold {fold + 1}/{cv_folds}...")
    
    X_fold_train, X_fold_val = X_array[train_idx], X_array[val_idx]
    y_fold_train, y_fold_val = y_array[train_idx], y_array[val_idx]
    
    # Convert to tensors
    X_train_tensor = torch.tensor(X_fold_train, dtype=torch.float32)
    X_val_tensor = torch.tensor(X_fold_val, dtype=torch.float32)
    
    if task_type == "classification" and output_size == 1:
        y_train_tensor = torch.tensor(y_fold_train, dtype=torch.float32).unsqueeze(1)
        y_val_tensor = torch.tensor(y_fold_val, dtype=torch.float32).unsqueeze(1)
    else:
        y_train_tensor = torch.tensor(y_fold_train, dtype=torch.long if task_type == "classification" else torch.float32)
        y_val_tensor = torch.tensor(y_fold_val, dtype=torch.long if task_type == "classification" else torch.float32)
    
    # Create fold model
    fold_model = MLP(X_fold_train.shape[1], output_size)
    fold_optimizer = optim.Adam(fold_model.parameters(), lr=lr)
    
    # Training loop
    fold_model.train()
    for epoch in range(epochs):
        fold_optimizer.zero_grad()
        output = fold_model(X_train_tensor)
        loss = criterion(output, y_train_tensor)
        loss.backward()
        fold_optimizer.step()
    
    # Evaluation
    fold_model.eval()
    with torch.no_grad():
        val_output = fold_model(X_val_tensor)
        
        if task_type == "classification":
            if output_size == 1:
                val_pred = (val_output > 0.5).float()
                fold_score = (val_pred.squeeze() == y_val_tensor.squeeze()).float().mean().item()
            else:
                val_pred = torch.argmax(val_output, dim=1)
                fold_score = (val_pred == y_val_tensor).float().mean().item()
        else:
            fold_score = nn.MSELoss()(val_output.squeeze(), y_val_tensor).item()
    
    cv_scores.append(fold_score)
    print(f"  Fold {fold + 1} {scoring_name}: {fold_score:.4f}")

mean_score = np.mean(cv_scores)
std_score = np.std(cv_scores)
print(f"\nMean CV {scoring_name}: {mean_score:.4f} ¬± {std_score:.4f}")

# Train final model on full data
print(f"\n[INFO] Training final model on full dataset...")
X_tensor = torch.tensor(X.values if hasattr(X, 'values') else X, dtype=torch.float32)
if task_type == "classification" and output_size == 1:
    y_tensor = torch.tensor(y.values if hasattr(y, 'values') else y, dtype=torch.float32).unsqueeze(1)
else:
    y_tensor = torch.tensor(y.values if hasattr(y, 'values') else y, dtype=torch.long if task_type == "classification" else torch.float32)

model.train()
for epoch in range(epochs):
    optimizer_torch.zero_grad()
    output = model(X_tensor)
    loss = criterion(output, y_tensor)
    loss.backward()
    optimizer_torch.step()
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}: Loss = {loss.item():.4f}")

{% else %}
# Prepare data for training
X_tensor = torch.tensor(X_train.values if hasattr(X_train, 'values') else X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values if hasattr(X_test, 'values') else X_test, dtype=torch.float32)

if task_type == "classification" and output_size == 1:
    y_tensor = torch.tensor(y_train.values if hasattr(y_train, 'values') else y_train, dtype=torch.float32).unsqueeze(1)
    y_test_tensor = torch.tensor(y_test.values if hasattr(y_test, 'values') else y_test, dtype=torch.float32).unsqueeze(1)
else:
    y_tensor = torch.tensor(y_train.values if hasattr(y_train, 'values') else y_train, dtype=torch.long if task_type == "classification" else torch.float32)
    y_test_tensor = torch.tensor(y_test.values if hasattr(y_test, 'values') else y_test, dtype=torch.long if task_type == "classification" else torch.float32)

dataset = TensorDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

print(f"Training on {len(dataloader)} batches...")

# Training loop
model.train()
for epoch in range(epochs):
    epoch_loss = 0
    for batch_idx, (data, target) in enumerate(dataloader):
        optimizer_torch.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_torch.step()
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(dataloader)
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}")

# Evaluation
model.eval()
with torch.no_grad():
    test_output = model(X_test_tensor)
    
    if task_type == "classification":
        if output_size == 1:
            test_pred = (test_output > 0.5).float()
            accuracy = (test_pred.squeeze() == y_test_tensor.squeeze()).float().mean()
            print(f"Test Accuracy: {accuracy:.4f}")
        else:
            test_pred = torch.argmax(test_output, dim=1)
            accuracy = (test_pred == y_test_tensor).float().mean()
            print(f"Test Accuracy: {accuracy:.4f}")
    else:
        mse = nn.MSELoss()(test_output.squeeze(), y_test_tensor)
        print(f"Test MSE: {mse:.4f}")
{% endif %}

{% endif %}

print("\n[SUCCESS] Training completed successfully!")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üé® VISUALIZATION GENERATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if VISUALIZATIONS_AVAILABLE:
    print("\n" + "="*60)
    print("üé® GENERATING VISUALIZATIONS")
    print("="*60)
    
    try:
        # Prepare CV scores if available
        cv_scores_for_viz = cv_scores.tolist() if 'cv_scores' in locals() else None
        
        # Generate visualizations
        saved_plots, visualizer = create_visualizations(
            model=model,
            X=X,
            y=y,
            model_name=model_name,
            task_type=task_type,
            cv_scores=cv_scores_for_viz
        )
        
        # Generate summary report
        model_info = {
            "model_name": model_name,
            "framework": "{{ framework }}",
            "dataset": dataset,
            "task_type": task_type,
            "cross_validation": {
                "enabled": use_cv,
                "folds": cv_folds if use_cv else None,
                "strategy": cv_type if use_cv else None
            }
        }
        
        if 'cv_scores' in locals():
            model_info["cv_results"] = {
                "mean_score": float(mean_score),
                "std_score": float(std_score),
                "individual_scores": cv_scores_for_viz,
                "scoring_metric": scoring_name
            }
        
        report_path = visualizer.generate_summary_report(model_info, saved_plots, model_name)
        
        print(f"\n‚úÖ Visualization complete! Generated {len(saved_plots)} plots")
        print(f"üìÅ Plots saved in: {visualizer.output_dir}")
        print(f"üìã Summary report: {report_path}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error generating visualizations: {e}")
        import traceback
        traceback.print_exc()

# Save model info and results
import json
import os

# Create outputs directory if it doesn't exist
os.makedirs("outputs", exist_ok=True)

# Prepare model info for JSON serialization
model_info_json = {
    "model_name": model_name,
    "framework": "{{ framework }}",
    "dataset": dataset,
    "task_type": task_type,
    "cross_validation": {
        "enabled": use_cv,
        "folds": cv_folds if use_cv else None,
        "strategy": cv_type if use_cv else None
    }
}

{% if use_cv %}
# Add CV results
if 'cv_scores' in locals():
    model_info_json["cv_results"] = {
        "mean_score": float(mean_score),
        "std_score": float(std_score),
        "individual_scores": [float(score) for score in cv_scores],
        "scoring_metric": scoring_name
    }
{% endif %}

{% if model_config %}
# Add model config if available
config_data = {
    "class": "{{ model_config.class }}",
    "framework": "{{ model_config.framework }}",
    "import": "{{ model_config.import }}",
    "default_params": {
        {% for key, value in model_config.default_params.items() %}
        "{{ key }}": {{ value | python_value }},
        {% endfor %}
    }
}
model_info_json["config"] = config_data
{% else %}
model_info_json["config"] = None
{% endif %}

# Add training parameters
model_info_json["training_params"] = {
    "learning_rate": lr,
    "epochs": epochs,
    "optimizer": optimizer,
    "other_params": {
        {% for key, value in other_params.items() %}
        "{{ key }}": {{ value | python_value }},
        {% endfor %}
    }
}

filename_suffix = "_cv" if use_cv else ""
with open(f"outputs/model_info_{model_name}{filename_suffix}.json", "w", encoding='utf-8') as f:
    json.dump(model_info_json, f, indent=2)

print(f"\nüíæ Model info saved to outputs/model_info_{model_name}{filename_suffix}.json")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üíæ MODEL SAVING FOR DEPLOYMENT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

print("\nüíæ Saving model for deployment...")

import pickle
import joblib
from datetime import datetime

# Create models directory
os.makedirs("outputs/models", exist_ok=True)

# Generate model filename
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
cv_suffix = "_cv" if use_cv else ""
model_filename = f"{model_name}_{dataset}{cv_suffix}_{timestamp}"

try:
    {% if framework == "sklearn" or framework == "xgboost" or framework == "lightgbm" or framework == "catboost" %}
    # Save sklearn-compatible models
    model_path = f"outputs/models/{model_filename}.pkl"
    joblib.dump(model, model_path)
    print(f"‚úÖ Model saved: {model_path}")
    
    {% elif framework == "tensorflow" %}
    # Save TensorFlow model
    model_path = f"outputs/models/{model_filename}.h5"
    model.save(model_path)
    print(f"‚úÖ TensorFlow model saved: {model_path}")
    
    {% elif framework == "pytorch" %}
    # Save PyTorch model
    model_path = f"outputs/models/{model_filename}.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'input_size': X.shape[1],
        'output_size': output_size,
        'task_type': task_type,
        'model_class': 'MLP'
    }, model_path)
    print(f"‚úÖ PyTorch model saved: {model_path}")
    {% endif %}
    
    # Save preprocessing components if they exist
    {% if framework in ["tensorflow", "pytorch"] or model in ["svm", "logistic_regression"] %}
    if 'scaler' in locals():
        scaler_path = f"outputs/models/{model_filename}_scaler.pkl"
        joblib.dump(scaler, scaler_path)
        print(f"‚úÖ Scaler saved: {scaler_path}")
    {% endif %}
    
    if label_encoders:
        encoders_path = f"outputs/models/{model_filename}_encoders.pkl"
        joblib.dump(label_encoders, encoders_path)
        print(f"‚úÖ Encoders saved: {encoders_path}")
        
    print(f"‚úÖ Model deployment files saved successfully!")
    
except Exception as e:
    print(f"‚ö†Ô∏è Error saving model: {e}")
    import traceback
    traceback.print_exc()

print("\nüéâ Training and visualization pipeline completed successfully!")